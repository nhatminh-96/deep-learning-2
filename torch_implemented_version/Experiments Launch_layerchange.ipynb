{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10588eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "from RBM import *\n",
    "from principal_DNN_mnist import *\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d3fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 16:47:51.865111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-17 16:47:51.865130: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data to fit input shape of model\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "# Turn images to binary images\n",
    "X_train = X_train > 127\n",
    "X_test  = X_test > 127\n",
    "\n",
    "#Original label to one hot coding label\n",
    "label_test = np.zeros((Y_test.shape[0],10))\n",
    "label_train = np.zeros((Y_train.shape[0],10))\n",
    "\n",
    "for i in range(len(Y_train)):\n",
    "    label_train[i, Y_train[i]] = 1\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    label_test[i, Y_test[i]] = 1\n",
    "\n",
    "Y_train = label_train\n",
    "Y_test = label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06bd3",
   "metadata": {},
   "source": [
    "# HERE TO CHOOSE SIZE OF TRAINING SET\n",
    "\n",
    "Original size of MNIST is 60000 in training set and 10000 in test set.\n",
    "\n",
    "Project asks us to test our model in training set of increasing size: 1000, 3000. 7000, 10000, 30000, 60000  \n",
    "\n",
    "So choosing training_size to have these sizes: 1/60, 1/20, 7/60, 1/6, 1/2, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9cda943",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabae291",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1.0\n",
    "# Data processing\n",
    "train_idx = np.random.randint(0, X_train.shape[0], int(X_train.shape[0]*training_size))\n",
    "test_idx = np.random.randint(0, X_test.shape[0], int(X_test.shape[0]*test_size))\n",
    "\n",
    "X_train = X_train[train_idx]\n",
    "Y_train = Y_train[train_idx]\n",
    "\n",
    "X_test  = X_test[test_idx]\n",
    "Y_test  = Y_test[test_idx]\n",
    "\n",
    "#########\n",
    "\n",
    "X_train = torch.from_numpy(X_train).to(device).float()\n",
    "Y_train = torch.from_numpy(Y_train).to(device).float()\n",
    "X_test = torch.from_numpy(X_test).to(device).float()\n",
    "Y_test = torch.from_numpy(Y_test).to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860edbf9",
   "metadata": {},
   "source": [
    "# HERE TO CHOOSE SIZE OF MODEL \n",
    "\n",
    "Size of model to be modify according to the project is number of neurons in each layer and number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c786957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layers = [150, 200] # To be modified. This is list of number of nodes in each layer (each value is a hidden layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082aec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting model hyper-parameters\n",
    "nb_epochs_pretrain = 200\n",
    "nb_epochs_scratch = 100\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "input_shape = X_train.shape[1]\n",
    "nb_class = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5a261",
   "metadata": {},
   "source": [
    "###### Training method 1: Construct a DNN model. First train it unsupervisedly and then fine-tune with supervised backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75c8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_1 = DNN(input_shape, nb_class, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e4f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_1.pretrain_DNN(X_train, nb_epochs_pretrain, lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae6ba7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_1, loss = retropropagation(DNN_method_1, input = X_train, label = Y_train, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4608f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_method_1 = test_DNN(DNN_method_1, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88f094",
   "metadata": {},
   "source": [
    "###### Training method 2: Construct a DNN model. Train it with supervised backpropagation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "382fd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_2 = DNN(input_shape, nb_class, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13037779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_2, loss = retropropagation(DNN_method_2, input = X_train, label = Y_train, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f86e513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_method_2 = test_DNN(DNN_method_2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fb760",
   "metadata": {},
   "source": [
    "### For each change of a hyperparameter of the model, we store accuracy_method_1 and accuracy_method_2 to plot these values in relation with chosen hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550529d4",
   "metadata": {},
   "source": [
    "Note: For this implementation of pytorch in model, sometimes, the accuracy is 0. I don't know why after a lot of research on the code (it seems that there is problem with weight initialization). In that case, re-train the model and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95dc95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [[200], [200, 200], [200, 200, 200], [200, 200, 200, 200], [200, 200, 200, 200, 200], [200, 200, 200, 200, 200, 200, 200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65200927",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d030c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching for hidden layers: [200]\n",
      "Training layer 0\n",
      "Training Started. 200 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [02:44<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Training\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 100/100 [00:29<00:00, 3.38it/s, loss =0.00009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "Accuracy: 0.9709\n",
      "Method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 100/100 [00:28<00:00, 3.47it/s, loss =0.09995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9114\n",
      "---------------\n",
      "Launching for hidden layers: [200, 200]\n",
      "Training layer 0\n",
      "Training Started. 200 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                           | 15/200 [00:12<02:32,  1.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching for hidden layers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhid_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m DNN_method_1 \u001b[38;5;241m=\u001b[39m DNN(input_shape, nb_class, hid_layer)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mDNN_method_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrain_DNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epochs_pretrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m DNN_method_1, loss \u001b[38;5;241m=\u001b[39m retropropagation(DNN_method_1, \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m X_train, label \u001b[38;5;241m=\u001b[39m Y_train, epochs \u001b[38;5;241m=\u001b[39m nb_epochs_scratch, lr \u001b[38;5;241m=\u001b[39m lr, batch_size \u001b[38;5;241m=\u001b[39m batch_size)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/M2/deep2/using torch/principal_DNN_mnist.py:46\u001b[0m, in \u001b[0;36mDNN.pretrain_DNN\u001b[0;34m(self, X, nb_epoch, lr, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_layers):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDNN\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_rbm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDNN[i]\u001b[38;5;241m.\u001b[39mentree_sortie_RBM(X_train)\n",
      "File \u001b[0;32m~/M2/deep2/using torch/RBM.py:165\u001b[0m, in \u001b[0;36mRBM.train_rbm\u001b[0;34m(self, x, iteration, lr, batch_size)\u001b[0m\n\u001b[1;32m    163\u001b[0m d_a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(v_0 \u001b[38;5;241m-\u001b[39m v_1, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    164\u001b[0m d_b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentree_sortie_RBM(v_0) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentree_sortie_RBM(v_1), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m d_W \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(v_0\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentree_sortie_RBM(v_0)) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(v_1\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentree_sortie_RBM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    167\u001b[0m lr_ \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m/\u001b[39mnb_samples\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate([lr_ \u001b[38;5;241m*\u001b[39m d_W, lr_ \u001b[38;5;241m*\u001b[39m d_a, lr_ \u001b[38;5;241m*\u001b[39md_b])\n",
      "File \u001b[0;32m~/M2/deep2/using torch/RBM.py:86\u001b[0m, in \u001b[0;36mRBM.entree_sortie_RBM\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03mThis function compute the hidden layer given the visible layer.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03mAccording to theory: P(h_j = 1 | v) = sigm(b_j + \\sum{w_{i,j} v_i}).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m            Our data\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# data_ = data.to(self.device)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m#z = data @ self.W + self.b\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#hidden = torch.sigmoid(z)\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for hid_layer in hidden_layers:\n",
    "    acc = []\n",
    "    print(f\"Launching for hidden layers: {hid_layer}\")\n",
    "    DNN_method_1 = DNN(input_shape, nb_class, hid_layer)\n",
    "    DNN_method_1.pretrain_DNN(X_train, nb_epochs_pretrain, lr, batch_size)\n",
    "    DNN_method_1, loss = retropropagation(DNN_method_1, input = X_train, label = Y_train, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)\n",
    "    print(\"Method 1\")\n",
    "    accuracy_method_1 = test_DNN(DNN_method_1, X_test, Y_test)\n",
    "    acc.append(accuracy_method_1[1])\n",
    "    print(\"Method 2\")\n",
    "    DNN_method_2 = DNN(input_shape, nb_class, hid_layer)\n",
    "    DNN_method_2, loss = retropropagation(DNN_method_2, input = X_train, label = Y_train, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)\n",
    "    accuracy_method_2 = test_DNN(DNN_method_2, X_test, Y_test)\n",
    "    acc.append(accuracy_method_2[1])\n",
    "    result[f\"{len(hid_layer)} layers of size {hid_layer[0]}\"]=acc\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e17d522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.016666666666666666: [0.893375, 0.844], 0.05: [0.93425, 0.90725]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43faa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result_datasize_change.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13b769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
