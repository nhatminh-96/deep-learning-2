{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10588eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "from RBM import *\n",
    "from principal_DNN_mnist import *\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d3fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 16:23:48.016301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-17 16:23:48.016373: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data to fit input shape of model\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\n",
    "\n",
    "# Turn images to binary images\n",
    "X_train = X_train > 127\n",
    "X_test  = X_test > 127\n",
    "\n",
    "#Original label to one hot coding label\n",
    "label_test = np.zeros((Y_test.shape[0],10))\n",
    "label_train = np.zeros((Y_train.shape[0],10))\n",
    "\n",
    "for i in range(len(Y_train)):\n",
    "    label_train[i, Y_train[i]] = 1\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    label_test[i, Y_test[i]] = 1\n",
    "\n",
    "Y_train = label_train\n",
    "Y_test = label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06bd3",
   "metadata": {},
   "source": [
    "# HERE TO CHOOSE SIZE OF TRAINING SET\n",
    "\n",
    "Original size of MNIST is 60000 in training set and 10000 in test set.\n",
    "\n",
    "Project asks us to test our model in training set of increasing size: 1000, 3000. 7000, 10000, 30000, 60000  \n",
    "\n",
    "So choosing training_size to have these sizes: 1/60, 1/20, 7/60, 1/6, 1/2, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cda943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aabae291",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.8\n",
    "# Data processing\n",
    "#train_idx = np.random.randint(0, X_train.shape[0], int(X_train.shape[0]*size))\n",
    "test_idx = np.random.randint(0, X_test.shape[0], int(X_test.shape[0]*test_size))\n",
    "\n",
    "#X_train = X_train[train_idx]\n",
    "#Y_train = Y_train[train_idx]\n",
    "\n",
    "X_test  = X_test[test_idx]\n",
    "Y_test  = Y_test[test_idx]\n",
    "\n",
    "#########\n",
    "\n",
    "# X_train = torch.from_numpy(X_train).to(device).float()\n",
    "# Y_train = torch.from_numpy(Y_train).to(device).float()\n",
    "X_test = torch.from_numpy(X_test).to(device).float()\n",
    "Y_test = torch.from_numpy(Y_test).to(device).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860edbf9",
   "metadata": {},
   "source": [
    "# HERE TO CHOOSE SIZE OF MODEL \n",
    "\n",
    "Size of model to be modify according to the project is number of neurons in each layer and number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c786957",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = [150, 200] # To be modified. This is list of number of nodes in each layer (each value is a hidden layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082aec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting model hyper-parameters\n",
    "nb_epochs_pretrain = 200\n",
    "nb_epochs_scratch = 100\n",
    "lr = 0.1\n",
    "batch_size = 64\n",
    "#input_shape = X_train.shape[1]\n",
    "nb_class = Y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b5a261",
   "metadata": {},
   "source": [
    "###### Training method 1: Construct a DNN model. First train it unsupervisedly and then fine-tune with supervised backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75c8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_1 = DNN(input_shape, nb_class, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e4f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_1.pretrain_DNN(X_train, nb_epochs_pretrain, lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6ba7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_1, loss = retropropagation(DNN_method_1, input = X_train, label = Y_train, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4608f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_method_1 = test_DNN(DNN_method_1, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88f094",
   "metadata": {},
   "source": [
    "###### Training method 2: Construct a DNN model. Train it with supervised backpropagation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "382fd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_2 = DNN(input_shape, nb_class, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13037779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN_method_2, loss = retropropagation(DNN_method_2, input = X_train, label = Y_train, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86e513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_method_2 = test_DNN(DNN_method_2, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fb760",
   "metadata": {},
   "source": [
    "### For each change of a hyperparameter of the model, we store accuracy_method_1 and accuracy_method_2 to plot these values in relation with chosen hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550529d4",
   "metadata": {},
   "source": [
    "Note: For this implementation of pytorch in model, sometimes, the accuracy is 0. I don't know why after a lot of research on the code (it seems that there is problem with weight initialization). In that case, re-train the model and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "289f84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [1./60, 1./20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65200927",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d030c47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching for train size: 1000\n",
      "Training layer 0\n",
      "Training Started. 200 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:02<00:00, 77.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Training\n",
      "--------------\n",
      "Training layer 1\n",
      "Training Started. 200 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 101.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Training\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 100/100 [00:00<00:00, 140it/s, loss =0.00076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "Accuracy: 0.89338\n",
      "Method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████| 100/100 [00:00<00:00, 141it/s, loss =nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.844\n",
      "---------------\n",
      "Launching for train size: 3000\n",
      "Training layer 0\n",
      "Training Started. 200 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:07<00:00, 26.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Training\n",
      "--------------\n",
      "Training layer 1\n",
      "Training Started. 200 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 34.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Training\n",
      "--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 100/100 [00:02<00:00, 48.6it/s, loss =0.00042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "Accuracy: 0.93425\n",
      "Method 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 100/100 [00:02<00:00, 48.4it/s, loss =nan]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90725\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in size:\n",
    "    acc = []\n",
    "    train_idx = np.random.randint(0, X_train.shape[0], int(X_train.shape[0]*s))\n",
    "    X_train_ = X_train[train_idx]\n",
    "    Y_train_ = Y_train[train_idx]\n",
    "    X_train_ = torch.from_numpy(X_train_).to(device).float()\n",
    "    Y_train_ = torch.from_numpy(Y_train_).to(device).float()\n",
    "    input_shape = X_train_.shape[1]\n",
    "    print(f\"Launching for train size: {int(X_train.shape[0]*s)}\")\n",
    "    DNN_method_1 = DNN(input_shape, nb_class, hidden_layers)\n",
    "    DNN_method_1.pretrain_DNN(X_train_, nb_epochs_pretrain, lr, batch_size)\n",
    "    DNN_method_1, loss = retropropagation(DNN_method_1, input = X_train_, label = Y_train_, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)\n",
    "    print(\"Method 1\")\n",
    "    accuracy_method_1 = test_DNN(DNN_method_1, X_test, Y_test)\n",
    "    acc.append(accuracy_method_1[1])\n",
    "    print(\"Method 2\")\n",
    "    DNN_method_2 = DNN(input_shape, nb_class, hidden_layers)\n",
    "    DNN_method_2, loss = retropropagation(DNN_method_2, input = X_train_, label = Y_train_, epochs = nb_epochs_scratch, lr = lr, batch_size = batch_size)\n",
    "    accuracy_method_2 = test_DNN(DNN_method_2, X_test, Y_test)\n",
    "    acc.append(accuracy_method_2[1])\n",
    "    result[s]=acc\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e17d522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.016666666666666666: [0.893375, 0.844], 0.05: [0.93425, 0.90725]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43faa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"result_datasize_change.json\", \"w\") as outfile:\n",
    "    json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f13b769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
